\section{Introduction}
In this chapter we introduce the idea of a stochastic control problem in one dimension, 
and construct a theoretical framework for the resolution of a regular solution, 
provided that such a solution exists, which is guarunteed under some conditions which
we enumerate in section \ref{sec:2.2}.

This chapter is mainly expository with the goal of setting us up 
to prove the results of \textcite{AS2008} in chapter \ref{chap:3}, and we follow 
the text of \textcite{Pham}. However, it is not a regurgitation of Pham's work. In particular,
for the proofs of key results such as the Dynamic Programming Principle (theorem \ref{thm:2.4.1})
and Hamiltoj-Jacobi-Bellman equation (theorem \ref{thm:2.5.1}) we fill in some of the jumps 
left by \textcite{Pham} with extra lines of working and more intuitive explanations. 

In section \ref{sec:2.2} 
we introduce the notion of a controlled diffusion process and its solution. In section 
\ref{sec:2.3} we consider a stochastic control problem over a finite time horizon 
before introducing the dynamic programming principle and Hamilton-Jacobi-Bellman 
equation in their finite-horizon variants in sections \ref{sec:2.4} and \ref{sec:2.5} 
respectively. Finally, in section \ref{sec:2.6}, we put these tools to use through a worked 
example in a financial context, setting us up to tackle the Avellaneda-Stoikov model in 
chapter \ref{chap:3}.

\section{Controlled Diffusion Processes}\label{sec:2.2}

In the previous chapter we have considered It\^{o} processes that are governed either by 
constants or by functions of time and/or state. Using this, we could for example model a 
stock price, the movement of a particle, or any other system with the kinds of properties 
that we study above. If we have a portfolio consiting of some cash and a position in an asset, 
we can model our wealth through time as a stochastic differential equation governed by the 
interest rate at which we deterministically earn returns on our cash, and the random 
fluctuations of the stock price. 

For the market-maker, however, this is insufficient. We described in section \ref{sec:1.2}
how a market maker might be able to influence the flow of orders they recieve over time, 
and hence their cash flow, by adjusting the limit bid and ask quotes that they send to the 
market. Hence, the market maker's portfolio value is governed by not only the fluctuations 
of the stock and the risk-free interest rate, but also (stochastically) by the spread that they set. 
We thus need a model that allows our diffusion process to be governed by not only functions 
of time and state, but also of some other process which we 
will call $\alpha$. 

Throughout this chapter we will assume the background of a standard continuous and filtered probability space
$(\Omega,\mathcal{F},(\mathcal{F}_t)_{t\geq0},\mathbb{P})$ as defined above in section \ref{sec:1.3}.

\begin{definition}[Controlled Diffusion Process]
    We consider a control model where the state of the system is governed by 
    an $\mathbb{R}$-valued SDE:
    \begin{equation}\label{eq:2.1}
        \mathrm dX_t=b(t,X_t,\alpha_t)\mathrm ds+\sigma(t,X_t,\alpha_t)\mathrm dW_t
    \end{equation}
    where $W$ is a standard Wiener process. The control $\alpha=(\alpha_t)$ is a 
    \emph{progressively measurable} process valued in $A\subseteq\mathbb{R}^m$.
\end{definition}

The functions $b:\mathbb{R}^+\times\mathbb{R}\times A\rightarrow\mathbb{R}$
and $\sigma:\mathbb{R}^+\times\mathbb{R}\times A\rightarrow\mathbb{R}$ are measurable
in all of their arguments and satisfy a uniform Lipschitz condition in A:
There exists a $K\geq0$ such that $\forall\;x,y\in\mathbb{R}, \forall\;a\in A,$
\begin{equation}\label{eq:2.2}
    |b(x,a)-b(y,a)|+|\sigma(x,a)-\sigma(y,a)|\leq K|x-y|.
\end{equation}
In what follows, for $0\leq t\leq T<\infty$, we denote by $\mathcal{T}_{t,T}$ the
set of \emph{stopping times} valued in $[t,T]$.

\section{The Finite-Horizon Problem}\label{sec:2.3}

Fix a finite horizon $0<T<\infty$. We denote by $\mathcal{A}$ the set of control
processes $\alpha$ such that for any arbitrary $x\in\mathbb{R},$
\begin{equation}\label{eq:2.3}
    \mathbb{E}\left[\int_0^T|b(x,\alpha_t)|^2+|\sigma(x,\alpha_t)|^2\mathrm dt\right]<\infty.
\end{equation}
From theorem \ref{thm:1.6.1}, conditions \eqref{eq:2.2} and \eqref{eq:2.3} ensure the existence
and uniqueness of a strong solution to the SDE \eqref{eq:2.1} starting from any initial
condition $(t,x)\in[0,T]\times\mathbb{R}$ and with any control process $\alpha\in\mathcal{A}$.
We denote this unique strong solution with almost surely continuous sample paths by 
$\{X_s^{t,x},t\leq s\leq T\}$.\\
Next we set out our functional objective. Let $f:[0,T]\times\mathbb{R}\times A\rightarrow\mathbb{R}$
and $g:\mathbb{R}\rightarrow\mathbb{R}$ be two measurable functions. We suppose that:

\begin{itemize}
    \item $g$ is lower-bounded \textbf{or}
    \item $g$ satisfies a quadratic growth condition: $|g(x)|\leq C(1+|x|^2)\;\forall\;x\in\mathbb{R}$ for some constant $C$ independent of $x$.
\end{itemize}

In our current context, $f$ will represent a kind of ``rolling'' reward function, 
which, when summed or integrated over time allows us to measure the payoff of 
our actions. The function $g$ represents a kind of terminal reward, for example, 
some kind of bonus for good performance received at the end of the time period.
We also denote by $\mathcal{A}(t,x)$ the subset of controls $\alpha\in\mathcal{A}$ 
such that

\begin{equation*}
    \mathbb{E}\left[\int_t^T|f(s,X_x^{t,x},\alpha_s)|\mathrm ds\right]<\infty
\end{equation*}

for $(t,x)\in[0,T]\times\mathbb{R}$, and we assume that this set is not empty for all 
$(t,x)\in[0,T]\times\mathbb{R}$. This integrability condition defines our set of 
\emph{admissible} controls, the set of possible values for the control that we will 
choose from. We now define the \emph{gain function} to be the expected value
of our cumulative rolling reward $f$ and terminal payoff $g$ under a particular 
control process $\alpha$:

\begin{definition}[Gain Function]
    \begin{equation}
        J(t,x,\alpha):=\mathbb{E}\left[\int_t^Tf(s,X_s^{t,x},\alpha_s)\mathrm ds+g(X_T^{t,x})\right]
    \end{equation}
    for all $(t,x)\in[0,T]\times\mathbb{R}$ and $\alpha\in\mathcal{A}(t,x).$
\end{definition}

Our objective is thus to maximise over possible control processes the gain function $J$,
and to do this we introduce the associated \emph{value function}:

\begin{definition}[Value Function]
    \begin{equation}\label{eq:2.6}
        v(t,x):=\sup_{\alpha\in\mathcal{A}(t,x)}J(t,x,\alpha).
    \end{equation}
\end{definition}

The value function represents the best possible 
gain function that we could achieve under what we will come to call an \emph{optimal}
control process $\alpha$. The concept behind much of stochastic control is that if 
we can find what this value function $v$ should be, then we can work backwards to determine 
the optimal $\alpha$.

\begin{definition}[Optimal control]
    Given an initial condition $(t,x)\in[0,T]\times\mathbb{R}$, we say that $\hat{\alpha}\in\mathcal{A}(t,x)$
    is an optimal control if
    \begin{equation*}
        v(t,x)=J(t,x,\hat{\alpha}).
    \end{equation*}
\end{definition}

\begin{remark}
    A control process $\alpha$ of the form $\alpha_s=a(s,X_s^{t,x})$ for some measurable
    function $a:[0,T]\times\mathbb{R}\rightarrow A$ is called a \emph{Markovian} control.
\end{remark}

\section{The Dynamic Programming Principle}\label{sec:2.4}

The Dynamic Programming Principle (DPP) is the fundamental tool upon which much of the 
theory of stochastic control relies. We formulate it as follows, considering only
the context of the finite-horizon problem described above.

\begin{theorem}[Dynamic Programming Principle]\label{thm:2.4.1}
    Let $(t,x)\in[0,T]\times\mathbb{R}.$ Then we have
    \begin{align}
        v(t,x)&=\sup_{\alpha\in\mathcal{A}(t,x)}\sup_{\theta\in\mathcal{T}_{t,T}}\mathbb{E}\left[\int_{t}^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right]\\
        &=\sup_{\alpha\in\mathcal{A}(t,x)}\inf_{\theta\in\mathcal{T}_{t,T}}\mathbb{E}\left[\int_{t}^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right].
    \end{align}
\end{theorem}
\begin{proof}[Proof of the DPP]
    By pathwise uniqueness of the SDE for X, for any admissible control $\alpha\in\mathcal{A}(t,x)$,
    for any $\theta\in\mathcal{T}_{t,T}$ and for all $s\geq\theta$
    \begin{equation*}
        X_s^{t,x}=X_s^{\theta,X_\theta^{t,x}}.
    \end{equation*}
    By the law of iterated expectations we then have
    \begin{align*}
        J(t,x,\alpha)&=\mathbb{E}\left[\int_t^Tf(s,X_s^{t,x},\alpha_s)\mathrm ds+g(X_T^{t,x})\right]\\
        &=\mathbb{E}\left[\mathbb{E}\left[\int_t^Tf(s,X_s^{t,x},\alpha_s)\mathrm ds+g(X_T^{t,x})|\mathcal{F}_\theta\right]\right]\\
        &=\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+\mathbb{E}\left[\int_\theta^Tf(s,X_s^{t,x},\alpha_s)\mathrm ds+g(X_T^{t,x})|\mathcal{F}_\theta\right]\right]\\
        &=\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+\mathbb{E}\left[\int_\theta^Tf(s,X_s^{t,x},\alpha_s)\mathrm ds+g(X_T^{t,x})\right]\right]\\
        &=\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+J(\theta,X_\theta^{t,x},\alpha)\right].
    \end{align*}
    Since $J(.,.,\alpha)\leq v$ and $\theta$ is arbitrary in $\mathcal{T}_{t,T}$,
    we obtain
    \begin{align*}
        J(t,x,\alpha)&\leq\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right]\\
        &\leq\inf_{\theta\in\mathcal{T}_{t,T}}\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right]\\
        &\leq\sup_{\alpha\in\mathcal{A}(t,x)}\inf_{\theta\in\mathcal{T}_{t,T}}\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right].
    \end{align*}
    By taking the supremum over $\alpha$ in the left hand side, we obtain the second
    of the desired inequalities:
    \begin{equation}
        v(t,x)\leq\sup_{\alpha\in\mathcal{A}(t,x)}\inf_{\theta\in\mathcal{T}_{t,T}}\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right].
    \end{equation}
    Next we fix an arbitrary control $\alpha\in\mathcal{A}(t,x)$ and $\theta\in\mathcal{T}_{t,T}.$
    By the definition of the value function, and the properties of the supremum and 
    of continuity, for any $\epsilon>0$ and $\omega\in\Omega$
    there exists an $\alpha^{\epsilon,\omega}\in\mathcal{A}(\theta(\omega),X_{\theta(\omega)}^{t,x}(\omega))$
    that is an $\epsilon$-optimal control for $v(\theta,X_{\theta(\omega)}^{t,x}(\omega))$, i.e.
    \begin{equation}\label{eq:2.13}
        v(\theta,X_{\theta(\omega)}^{t,x}(\omega))-\epsilon \leq J(\theta(\omega),X_{\theta(\omega)}^{t,x}(\omega),\alpha^{\epsilon,\omega}).
    \end{equation}
    We now define the process
    \begin{equation*}
        \hat{\alpha}_s(\omega)=\begin{cases}
            &\alpha_s(\omega), s\in[0,\theta(\omega)]\\
            &\alpha_s^{\epsilon,\omega}(\omega),s\in[\theta(\omega),T]
        \end{cases}
    \end{equation*}
    It can be shown by the measurable selection theorem that the process 
    $\hat{\alpha}$ is progressively measurable, and so lies in $\mathcal{A}(t,x)$.
    A proof of this can be found in chapter 7 of \textcite{BeSh78}.
    Again by the law of iterated expectations and \eqref{eq:2.13} we get 
    \begin{align*}
        v(t,x)\geq J(t,x,\hat{\alpha})=\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+J(\theta,X_\theta^{t,x},\alpha^\epsilon)\right]\\
        \geq\mathbb{E}\left[\int_t^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right]-\epsilon.
    \end{align*}
    Finally, by the fact that $\alpha\in\mathcal{A}(t,x),\theta\in\mathcal{T}_{t,T}$
    and $\epsilon>0$ are all arbitrary, we obtain the first inequality:
    \begin{equation}
        v(t,x)=\sup_{\alpha\in\mathcal{A}(t,x)}\sup_{\theta\in\mathcal{T}_{t,T}}\mathbb{E}\left[\int_{t}^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right].
    \end{equation}
\end{proof}

\begin{remark}[Equivalent Formulations]
    We normally write the DPP as 
    \begin{equation}\label{eq:2.16}
        v(t,x)=\sup_{\alpha\in\mathcal{A}(t,x)}\mathbb{E}\left[\int_{t}^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right],
    \end{equation}
    however, it is sometimes useful to use the following equivalent formulation of the DPP:\\
    (i) For all $\alpha\in\mathcal{A}(t,x)$ and $\theta\in\mathcal{T}_{t,T}$:
    \begin{equation}\label{eq:2.17}
        v(t,x)\geq\mathbb{E}\left[\int_{t}^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right].
    \end{equation}
    (ii) For all $\epsilon>0,$ there exists $\alpha\in\mathcal{A}(t,x)$ such that for all $\theta\in\mathcal{T}_{t,T}$:
    \begin{equation}
        v(t,x)-\epsilon\leq\mathbb{E}\left[\int_{t}^\theta f(s,X_s^{t,x},\alpha_s)\mathrm ds+v(\theta,X_\theta^{t,x})\right].
    \end{equation}
\end{remark}

The idea behind the DPP is that we do not have to solve for the optimal control over 
the entire time interval in one go. We can split the problem into two, firstly finding 
an optimal control from $\theta$ to $T$, and secondly finding the optimal control from $t$ up
to $\theta$, as given in \eqref{eq:2.16}. The important fact here is that we are not limited 
to two sub-problems. We can continue splitting up our interval of time into increasingly 
small chunks, hopefully making each subproblem more tractable.

\section{Hamilton-Jacobi-Bellman Equation}\label{sec:2.5}

The Dynamic Programming Principle tells us that we can consider a stochastic control
problem as a sequence of smaller sub-problems defined over intervals of $[0,T]$
characterised by stopping times, i.e., $[0,T]=[0,\theta_1]\cup(\theta_1,\theta_2]\cup\dots\cup(\theta_{n},T]$
where $\theta_1\leq\dots\leq\theta_n\in\mathcal{T}_{t,T}.$ Thus, a natural thing to consider
is the following: What happens as $n\rightarrow\infty$ and correspondingly $\theta_{i+1}-\theta_i\rightarrow0$?
What we obtain is the Hamilton-Jacobi-Bellman equation (HJB) which describes the 
dynamics of the value function over small increments of time. In this chapter and what 
follows, we will use the HJB equation as follows:
\begin{itemize}
    \item Provide a formal derivation of the HJB equation.
    \item Obtain or try to show the existence of a smooth solution.
    \item Verification step: Show that the smooth solution is the value function.
    \item As a byproduct, we obtain an optimal feedback control.
\end{itemize}

\begin{theorem}[Hamilton-Jacobi-Bellman Equation]\label{thm:2.5.1}
    The dynamics of the value function $v(t,x)$ satisfy the following non-linear
    second-order partial differential equation:
    \begin{equation}\label{eq:2.19}
        \begin{aligned}
            &\frac{\partial v}{\partial t}(t,x)+\sup_{a\in A}\left[\mathcal{L}^av(t,x)+f(t,x,a)\right]=0\;\forall\;(t,x)\in[0,T)\times\mathbb{R},\\
            &v(T,x)=g(x)\;\forall\;x\in\mathbb{R}.
        \end{aligned}
    \end{equation}
    where $\mathcal{L}^a$ is the operator associated to the diffusion \eqref{eq:2.1}
    and defined by (see \eqref{eq:1.14}) from section \ref{sec:1.6}
    \begin{equation*}
        \mathcal{L}^av=b(t,x,a)v_x+\frac{1}{2}\sigma(t,x,a)^2v_{xx}.
    \end{equation*}
\end{theorem}
\begin{proof}
    Let us consider time $\theta=t+h$ and a constant control $\alpha_s=a$ for some arbitrary
    $a\in A$, in our slightly stronger variant of the DPP \eqref{eq:2.17}:
    \begin{equation}\label{eq:2.21}
        v(t,x)\geq\mathbb{E}\left[\int_{t}^{t+h} f(s,X_s^{t,x},a)\mathrm ds+v(t+h,X_{t+h}^{t,x})\right].
    \end{equation}
    By assuming that $v$ is smooth enough, we can apply It\^{o}'s formula between
    $t$ and $t+h$:
    \begin{equation*}
        v(t+h,X_{t+h}^{t,x})=v(t,x)+\int_t^{t+h}\left(\frac{\partial v}{\partial t}+\mathcal{L}^av\right)(s,X_s^{t,x})\mathrm ds.
    \end{equation*}
    We can then substitute back into \eqref{eq:2.21} to obtain
    \begin{equation*}
        0\geq\mathbb{E}\left[\int_t^{t+h}\left(\frac{\partial v}{\partial t}+\mathcal{L}^av\right)(s,X_s^{t,x})+f(s,X_s^{t,x},a)\mathrm ds\right]
    \end{equation*}
    which if we divide by $h$ and send $h\rightarrow0$ we yield
    \begin{equation*}
        0\geq\frac{\partial v}{\partial t}(t,x)+\mathcal{L}^av(t,x)+f(t,x,a)
    \end{equation*}
    by the mean-value theorem. Since this holds true for any $a\in A$, we obtain the
    inequality
    \begin{equation}\label{eq:2.25}
        -\frac{\partial v}{\partial t}(t,x)-\sup_{a\in A}[\mathcal{L}^av(t,x)+f(t,x,a)]\geq0.
    \end{equation}
    On the other hand, suppose that $\alpha^*$ is an optimal control. Then in \eqref{eq:2.16}
    we have
    \begin{equation}\label{eq:2.26}
        v(t,x)=\mathbb{E}\left[\int_t^{t+h}f(s,X_s^*,\alpha_s^*)\mathrm ds+v(t+h,X_{t+h}^*)\right],
    \end{equation}
    where $X^*$ is the solution to \eqref{eq:2.1} starting from state $x$ at time $t$
    with control $\alpha^*.$ Again by It\^{o}'s formula we have that 
    \begin{equation*}
        v(t+h,X^*_{t+h})=v(t,x)+\int_t^{t+h}\left(\frac{\partial v}{\partial t}+\mathcal{L}^av\right)(s,X_s^*)\mathrm ds
    \end{equation*}
    which we can again substitute back into \eqref{eq:2.26} to obtain
    \begin{equation*}
        0=\mathbb{E}\left[\int_t^{t+h}\left(\frac{\partial v}{\partial t}+\mathcal{L}^av\right)(s,X_s^*)+f(s,X_s*,a)\mathrm ds\right]
    \end{equation*}
    and hence once again we divide by $h$ and send $h\rightarrow0$ yielding
    \begin{equation*}
        -\frac{\partial v}{\partial t}(t,x)-\mathcal{L}^{\alpha^*_t}v(t,x)-f(t,x,\alpha_t^*)=0.
    \end{equation*}
    Combining this with \eqref{eq:2.25}, $v$ should satisfy
    \begin{equation}
        -\frac{\partial v}{\partial t}(t,x)-\sup_{a\in A}\left[\mathcal{L}^av(t,x)+f(t,x,a)\right]=0\;\forall\;(t,x)\in[0,T)\times\mathbb{R},
    \end{equation}
    if the above supremum in $a$ is finite. For the purpose of this report, we will 
    assume that this is always true for the scenarios we will deal with. We can also 
    obtain the terminal condition associated to this PDE:
    \begin{equation}
        v(T,x)=g(x)\;\forall\;x\in\mathbb{R}
    \end{equation}
    which results immediately from the definition in \eqref{eq:2.6} of the value function
    considered at the horizon time $T$.
\end{proof}

W general process of solving a stochastic optimal control problem is the 
following:
\begin{enumerate}
    \item Formulate the problem in terms of a controlled diffusion process
    \item Define the value function 
    \item Formulate the HJB equation that describes the dynamics 
    of the value function
    \item Solve the HJB equation to obtain an exact form or approximation for the 
    value function
    \item Utilise the resolved value function to find the optimal control process
\end{enumerate}

There is an extra step that we have not covered in chapter \ref{chap:2},
namely the \emph{verification theorem}. This theorem provides necessary and sufficient 
conditions for the candidate value function found in step 4. and candidate optimal 
control found in step 5. to constitute the full solution to the problem. For simplicity,
we do not include it here as we will not be explicitly solving the HJB equation associated 
to our value function for the market-making problem in chapter \ref{chap:3}, however,
the theorem and its proof is included in appendix \ref{app:A}.

To finish off chapter \ref{chap:2}, we will work through a famous example, given 
by \textcite{MER69}, of portfolio allocation over a finite time horizon in a 
Black-Scholes-Merton Model.

\section{Finite-Horizon Merton Portfolio Allocation Problem}\label{sec:2.6}

Consider an agent who, at time $t$, invests a proportion of her wealth $\alpha_t$
in a stock of price $S$ governed by a geometric Brownian motion, and $1-\alpha_t$
in a riskless bond of price $S^0$ with interest rate r. We consider a finite horizon
$[0,T]$, and at any time $t\in[0,T]$ the investor faces the constraint that $\alpha_t$
is valued in $A$, a closed convex subset of $\mathbb{R}$ (namely, $A=[0,1]$).

The wealth process evolves according to 
\begin{align*}
    \mathrm dX_t&=\frac{X_t\alpha_t}{S_t}\mathrm dS_t+\frac{X_t(1-\alpha_t)}{S^0_t}\mathrm dS_t^0\\
    &=X_t(\alpha_t\mu+(1-\alpha_t)r)\mathrm dt+X_t\alpha_t\sigma\mathrm dW_t.
\end{align*}

Denote by $\mathcal{A}$ the set of progressively measurable processes $\alpha$ 
valued in $A$ and such that 
\begin{equation*}
    \int_0^T\alpha_s^2\mathrm ds<\infty\textrm{ a.s.}
\end{equation*}
This integrability condition ensures the existence and uniqueness of a strong solution
to the SDE governing the wealth process controlled by $\alpha$. Given a strategy $\alpha$,
we denote by $X^{t,x}$ the corresponding wealth process starting from initial capital
$X_t=x>0$ at time $t$. The agent wants to maximise the expected utility of terminal wealth,
giving us the value function 
\begin{equation}
    v(t,x)=\sup_{\alpha\in\mathcal{A}}\mathbb{E}[U(X_T^{t,x})],\;(t,x)\in[0,T]\times\mathbb{R}^+.
\end{equation}
The HJB equation for this stochastic control problem is given by 
\begin{equation}\label{eq:2.33}
    \frac{\partial v}{\partial t}+\sup_{a\in A}\{\mathcal{L}^av(t,x)\}=0,
\end{equation}
together with the terminal condition
\begin{equation}\label{eq:2.34}
    v(T,x)=U(x),\;x\in\mathbb{R}^+.
\end{equation}
Here, 
\begin{equation}\label{eq:2.35}
    \mathcal{L}^av(t,x)=x(a\mu+(1-a)r)\frac{\partial v}{\partial x}+\frac{1}{2}x^2a^2\sigma^2\frac{\partial^2 v}{\partial x^2}.
\end{equation}
As originally considered by Merton, we will look at power utility functions of 
Constant Relative Risk Aversion (CRRA) type, taking the form
\begin{equation*}
    U(x)=\frac{x^p}{p},\;x\geq0,p<1,p\neq0.
\end{equation*}
It turns out that explicit smooth solutions do exist for the problem given by 
\eqref{eq:2.33}-\eqref{eq:2.34}. We are looking for a candidate solution of the 
form 
\begin{equation*}
    v(t,x)=\phi(t)U(x)
\end{equation*}
for some positive function $\phi$. By substituting into \eqref{eq:2.33}-\eqref{eq:2.34},
we derive that $\phi$ should satisfy the ODE
\begin{equation}
    \begin{aligned}
        &\phi^\prime(t)+\rho\phi(t)=0\\
        &\phi(T)=1
    \end{aligned}
\end{equation}
where 
\begin{equation*}
    \rho=p\sup_{a\in A}\{a(\mu-r)+r-\frac{1}{2}a^2(1-p)\sigma^2\}.
\end{equation*}
This ODE is solved by $\phi(t)=e^{\rho(T-t)}$. Hence we arrive at 
\begin{equation}\label{eq:2.36}
    v(t,x)=e^{\rho(T-t)}U(x),\;(t,x)\in[0,T]\times\mathbb{R}^+.
\end{equation}
Moreover, $a(\mu-r)+r-\frac{1}{2}a^2(1-p)\sigma^2$ is a concave function of $a\in A$
and thus attains its maximum at some point $\hat a$. By construction, $\hat a$ also
attains the supremum of $\sup_{a\in A}\{\mathcal{L}^av(t,x)\}$. Moreover, the wealth
process associated to the constant control $\hat a$
\begin{equation*}
    \mathrm dX_t=X_t(\hat a\mu+(1-\hat a)r)\mathrm dt+X_t\hat a\sigma\mathrm dW_t
\end{equation*}
admits a unique solution given an initial condition. We can also use the verification 
theorem in appendix \ref{app:A} to prove that the value function in problem \eqref{eq:2.33}
is equal to \eqref{eq:2.35}, and the optimal proportion of wealth to invest is given 
by $\hat a$. Finally, using the concavity of 
\begin{equation*}
    a(\mu-r)+r-\frac{1}{2}a^2(1-p)\sigma^2
\end{equation*}
in $a$ we obtain that 
\begin{equation}
    \hat a=\frac{\mu-r}{(1-p)\sigma^2}
\end{equation}
and 
\begin{equation}
    \rho=\frac{(\mu-r)^2}{2\sigma^2}\frac{p}{1-p}+rp.
\end{equation}

Having now concretely demonstrated an application of stochastic optimal control to a financial 
problem, we will return in chapter \ref{chap:3} to the market-making problem introduced in
section \ref{sec:1.2}, and constructing the model given by \textcite{AS2008}.
